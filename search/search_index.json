{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Life insurance demonstration \u00b6 This repository includes a simple example of how to integrate, with minimum disruption, IBM Event Streams (Kafka) with an existing MQ based framework which manages messages distribution between MQ applications. The example is in life insurance domain but could be applied in other industry. Architecture context \u00b6 The existing solution integrate a queueing framework that receives messages from different applications (front end / mobile for the most part) and addresses message distribution, message transformation, error management, retry, notification, data life cycle auditing and governance. The ask is to see how event-driven architecture will help to support some of the capability of the framework or complement it, and how it can help propagate message as events to event-driven microservices. Figure below illustrates a generic view of how the existing framework is running: At the top we have different front end applications that can send transactional data (write to life insurance model), or not transactional ones The APIs consumed by the frontend could be mediated with ESB (IBM IIB) flows, and then some of those flows are publishing messages to IBM MQ queues. From those queues, we can get different processing running all together to do data enrichment, transformation, to get subscriber applications consuming those data. Other services are responsible to do retries, auditing, manage errors, or notify end user with a mobile push or email back-end An important component of this framework is the transaction event sourcing capability: keep state of change on some interesting transactional data: for example a life insurance offer. Different flows are doing the needed works, and all this framework is basically supporting long running transaction processing and a notification engine. Also note, that to be generic this framework defines different message types (600) and adapt mediation flow via configuration. The solution in on bare metal or VM running on premise. Requirements to demonstrate \u00b6 Address how to extend existing architecture with Kafka based middleware and streaming processing. (See next section ) Demonstrate streaming processing with the exactly once delivery (See transaction streaming component ) Ensure Event order is not changed: in the queuing approach with subscription, it is possible that a message arrived after another one could be processed before the first one is completed, which could impact data integrity. Demonstrate Data transformation to target different models, to prepare the data for a specific subscriber (a kafka consumer). ( See this streaming code ) Support message content based routing Dead letter queue support for data in error Support CloudEvent.io to present metadata around the message Support Schema management in registry to control the definition of the message in a unique central repository Demonstrate access control to topic See user declaration . Example of non-desruptive integration \u00b6 The existing framework can be extended by adding Kafka MQ source and sink connectors in parallel of existing framework, with deploy Kafka based middleware (IBM Event Streams): transactional or non-transactional data will injected from queues to different Kafka Topics as event ready to be processed as soon as created. For the streaming processing, we propose to do data enrichment, data validation to route erranous data to dead-letter-queue topic, and data transformation to publish to two different topics for downstream subscribers: these will validate content based routing and enrichment, and exactly once delivery with order guarantee. Subscriber applications illustrate in the figure above could be new applications, or existing one connected to Kafka directly or if actually connected to Queue, then those subscribers will be MQ Sink kafka connector. Read more To understand Kafka topic - offset see this note Kafka MQ Source connector lab Dead letter queue pattern Domain model \u00b6 We pick up the life insurance domain, but as of now very limited, it could be extended in the future. See the design section Components \u00b6 We leverage the following IBM Products: Event Streams with one cluster definition is in this eventstreams-dev yaml MQ broker with AMQP protocol enabled (See this folder for deployment example) Kafka Connector Event end point management Schema registry And develop three components to demonstrate how to support requirements: A transaction simulator to send data to MQ to support different demonstration goals. The app is done in Java Messaging Service in lf-tx-simulator folder . This application also send categories at startup time. a Kafka streams processing app which is using standard Java Kafka Streams API. The application is in client-event-processing folder Configuration for MQ source connector. The Yaml file is in environments mq-source folder >>> Next: Read more why EDA, fit for purpose... More reading Building reactive Java apps with Quarkus and IBM MQ Kafka Streams with a beginner guider Quarkus AMQP 1.0 Quickstart","title":"Introduction"},{"location":"#life-insurance-demonstration","text":"This repository includes a simple example of how to integrate, with minimum disruption, IBM Event Streams (Kafka) with an existing MQ based framework which manages messages distribution between MQ applications. The example is in life insurance domain but could be applied in other industry.","title":"Life insurance demonstration"},{"location":"#architecture-context","text":"The existing solution integrate a queueing framework that receives messages from different applications (front end / mobile for the most part) and addresses message distribution, message transformation, error management, retry, notification, data life cycle auditing and governance. The ask is to see how event-driven architecture will help to support some of the capability of the framework or complement it, and how it can help propagate message as events to event-driven microservices. Figure below illustrates a generic view of how the existing framework is running: At the top we have different front end applications that can send transactional data (write to life insurance model), or not transactional ones The APIs consumed by the frontend could be mediated with ESB (IBM IIB) flows, and then some of those flows are publishing messages to IBM MQ queues. From those queues, we can get different processing running all together to do data enrichment, transformation, to get subscriber applications consuming those data. Other services are responsible to do retries, auditing, manage errors, or notify end user with a mobile push or email back-end An important component of this framework is the transaction event sourcing capability: keep state of change on some interesting transactional data: for example a life insurance offer. Different flows are doing the needed works, and all this framework is basically supporting long running transaction processing and a notification engine. Also note, that to be generic this framework defines different message types (600) and adapt mediation flow via configuration. The solution in on bare metal or VM running on premise.","title":"Architecture context"},{"location":"#requirements-to-demonstrate","text":"Address how to extend existing architecture with Kafka based middleware and streaming processing. (See next section ) Demonstrate streaming processing with the exactly once delivery (See transaction streaming component ) Ensure Event order is not changed: in the queuing approach with subscription, it is possible that a message arrived after another one could be processed before the first one is completed, which could impact data integrity. Demonstrate Data transformation to target different models, to prepare the data for a specific subscriber (a kafka consumer). ( See this streaming code ) Support message content based routing Dead letter queue support for data in error Support CloudEvent.io to present metadata around the message Support Schema management in registry to control the definition of the message in a unique central repository Demonstrate access control to topic See user declaration .","title":"Requirements to demonstrate"},{"location":"#example-of-non-desruptive-integration","text":"The existing framework can be extended by adding Kafka MQ source and sink connectors in parallel of existing framework, with deploy Kafka based middleware (IBM Event Streams): transactional or non-transactional data will injected from queues to different Kafka Topics as event ready to be processed as soon as created. For the streaming processing, we propose to do data enrichment, data validation to route erranous data to dead-letter-queue topic, and data transformation to publish to two different topics for downstream subscribers: these will validate content based routing and enrichment, and exactly once delivery with order guarantee. Subscriber applications illustrate in the figure above could be new applications, or existing one connected to Kafka directly or if actually connected to Queue, then those subscribers will be MQ Sink kafka connector. Read more To understand Kafka topic - offset see this note Kafka MQ Source connector lab Dead letter queue pattern","title":"Example of non-desruptive integration"},{"location":"#domain-model","text":"We pick up the life insurance domain, but as of now very limited, it could be extended in the future. See the design section","title":"Domain model"},{"location":"#components","text":"We leverage the following IBM Products: Event Streams with one cluster definition is in this eventstreams-dev yaml MQ broker with AMQP protocol enabled (See this folder for deployment example) Kafka Connector Event end point management Schema registry And develop three components to demonstrate how to support requirements: A transaction simulator to send data to MQ to support different demonstration goals. The app is done in Java Messaging Service in lf-tx-simulator folder . This application also send categories at startup time. a Kafka streams processing app which is using standard Java Kafka Streams API. The application is in client-event-processing folder Configuration for MQ source connector. The Yaml file is in environments mq-source folder >>> Next: Read more why EDA, fit for purpose... More reading Building reactive Java apps with Quarkus and IBM MQ Kafka Streams with a beginner guider Quarkus AMQP 1.0 Quickstart","title":"Components"},{"location":"demo/","text":"Demonstration Script \u00b6 Goal \u00b6 The goal of the demonstration is to send two messages to illustrate client creation and update message in this order. Then each message is enriched and routed to different topics. The sequence of business operations: Bob TheBuilder is a new client so a web app is creating the record via a POST operation to the client management simulator The client record is updated by adding a beneficiary as spouce Then email address is changed. We can do two type of environment: Local on your laptop using docker compose Once deployed on OpenShift with Event Streams and MQ Local execution demonstration \u00b6 Pre-requisites You have docker desktop or a compatible product installed on your laptop. Under the project top folder starts docker compose: # life-insurance-demo docker-compose up -d Verify the existing topics are empty, using the open source tool called Kafdrop: chrome http://localhost:9000/ When the simulator starts, it sends the categories as reference data to the lf-category topic, which you can verify in Kafdrop Configure Kafka MQ Source connector so data sent by the Simulator to MQ are moved to Kafka lf-raw-tx topic: ./environments/local/sendMQSrcConfig.sh Result should look like: [ \"mq-source\" ] 20 To access the MQ Console use the following URL chrome https://localhost:9443 You shoud reach a login page (admin/passw0rd) and then the Home page Send a new client creation command: which does a HTTP POST to the simulator URL /api/v1/clients . # under home ./e2e/local/sendTxToSimulator.sh ./e2e/data/client-julie.json Verify the data is now in the lf-raw-tx The Stream processing has routed the transaction to the lf-tx-a topic Create a second transaction with one error (the category id is wrong), the message is routed to the dead letter queue ./e2e/local/sendTxToSimulator.sh ./e2e/data/client-bob-1.json Modify the client with a business category ./e2e/local/sendTxToSimulator.sh ./e2e/data/client-bob-2.json All the transactions are kept in order to reprocess if needed: Stop your environment docker compose down OpenShift Deployment demonstration \u00b6 Pre-requisites You have te make tool Deploy the solution with one commmand cd environments make all See more detail in this section Verify your environments Verify MQ source connector is ready oc get kafkaconnectors # Output> # NAME CLUSTER CONNECTOR CLASS MAX TASKS READY # mq-source eda-kconnect-cluster com.ibm.eventstreams.connect.mqsource.MQSourceConnector 1 True Deeper view of the connector oc describe kafkaconnector mq-source Access the Simulator App chrome Access the Simulator APIs by clicking on q/swagger-ui link from the home page: Use the POST on /api/v1/clients to send a client creation command with the following data: \"id\" : \"101012\" , \"code\" : \"C02\" , \"insuredPerson\" : { \"id\" : 2 , \"code\" : \"P02\" , \"first_name\" : \"julie\" , \"last_name\" : \"thesimmer\" , \"address\" : \"10 market street, CA, San Franciso\" , \"phone\" : \"650-650-650\" , \"mobile\" : \"\" , \"email\" : \"jswimmer@email.com\" }, \"client_category_id\" : 1 The request: The response with a populated id: Verify the message reaches the Kafka topic named lf-raw-tx >>> Next: OpenShift Deployment detail","title":"Demonstration"},{"location":"demo/#demonstration-script","text":"","title":"Demonstration Script"},{"location":"demo/#goal","text":"The goal of the demonstration is to send two messages to illustrate client creation and update message in this order. Then each message is enriched and routed to different topics. The sequence of business operations: Bob TheBuilder is a new client so a web app is creating the record via a POST operation to the client management simulator The client record is updated by adding a beneficiary as spouce Then email address is changed. We can do two type of environment: Local on your laptop using docker compose Once deployed on OpenShift with Event Streams and MQ","title":"Goal"},{"location":"demo/#local-execution-demonstration","text":"Pre-requisites You have docker desktop or a compatible product installed on your laptop. Under the project top folder starts docker compose: # life-insurance-demo docker-compose up -d Verify the existing topics are empty, using the open source tool called Kafdrop: chrome http://localhost:9000/ When the simulator starts, it sends the categories as reference data to the lf-category topic, which you can verify in Kafdrop Configure Kafka MQ Source connector so data sent by the Simulator to MQ are moved to Kafka lf-raw-tx topic: ./environments/local/sendMQSrcConfig.sh Result should look like: [ \"mq-source\" ] 20 To access the MQ Console use the following URL chrome https://localhost:9443 You shoud reach a login page (admin/passw0rd) and then the Home page Send a new client creation command: which does a HTTP POST to the simulator URL /api/v1/clients . # under home ./e2e/local/sendTxToSimulator.sh ./e2e/data/client-julie.json Verify the data is now in the lf-raw-tx The Stream processing has routed the transaction to the lf-tx-a topic Create a second transaction with one error (the category id is wrong), the message is routed to the dead letter queue ./e2e/local/sendTxToSimulator.sh ./e2e/data/client-bob-1.json Modify the client with a business category ./e2e/local/sendTxToSimulator.sh ./e2e/data/client-bob-2.json All the transactions are kept in order to reprocess if needed: Stop your environment docker compose down","title":"Local execution demonstration"},{"location":"demo/#openshift-deployment-demonstration","text":"Pre-requisites You have te make tool Deploy the solution with one commmand cd environments make all See more detail in this section Verify your environments Verify MQ source connector is ready oc get kafkaconnectors # Output> # NAME CLUSTER CONNECTOR CLASS MAX TASKS READY # mq-source eda-kconnect-cluster com.ibm.eventstreams.connect.mqsource.MQSourceConnector 1 True Deeper view of the connector oc describe kafkaconnector mq-source Access the Simulator App chrome Access the Simulator APIs by clicking on q/swagger-ui link from the home page: Use the POST on /api/v1/clients to send a client creation command with the following data: \"id\" : \"101012\" , \"code\" : \"C02\" , \"insuredPerson\" : { \"id\" : 2 , \"code\" : \"P02\" , \"first_name\" : \"julie\" , \"last_name\" : \"thesimmer\" , \"address\" : \"10 market street, CA, San Franciso\" , \"phone\" : \"650-650-650\" , \"mobile\" : \"\" , \"email\" : \"jswimmer@email.com\" }, \"client_category_id\" : 1 The request: The response with a populated id: Verify the message reaches the Kafka topic named lf-raw-tx >>> Next: OpenShift Deployment detail","title":"OpenShift Deployment demonstration"},{"location":"deployment/","text":"OpenShift Deployment \u00b6 We assume you have already Cloud Pak for Integration installed, and you are already logged to the OpenShift console. Pre-requisites \u00b6 make tool oc CLI Logged to your OpenShift cluster Using make \u00b6 We have a makefile to drive the installation of the demonstration components cd environments make all Example of output namespace/lf-demo created serviceaccount/lf-demo-sa created clusterrolebinding.rbac.authorization.k8s.io/secrets-to-sa created job.batch/cpsecret created --------------------------- ls-demo project created. Using project \"lf-demo\" on server \"https://api.ahsoka.coc-ibm.com:6443\" . Using project \"lf-demo\" on server \"https://api.ahsoka.coc-ibm.com:6443\" . ------------------------------------------------ Create dev Event Streams cluster in lf-demo project ------------------------------------------------ eventstreams.eventstreams.ibm.com/dev created kafkatopic.eventstreams.ibm.com/lf-raw-tx created kafkatopic.eventstreams.ibm.com/lf-tx-a created kafkatopic.eventstreams.ibm.com/lf-tx-b created kafkauser.eventstreams.ibm.com/scram-user created kafkauser.eventstreams.ibm.com/tls-user created ------------------------------------------------ Create dev IBM MQ in lf-demo project ------------------------------------------------ configmap/mq-config created configmap/mq-mqsc-config created queuemanager.mq.ibm.com/lf-demo-mq created ------------------------------------------------ Create dev Kafka Connect cluster in lf-demo project ------------------------------------------------ kafkaconnect.eventstreams.ibm.com/eda-kconnect-cluster created ------------------------------------------------ Deploy MQ Source connector ------------------------------------------------ kafkaconnector.eventstreams.ibm.com/mq-source created configmap/lf-tx-simulator-cm created service/lf-tx-simulator created deployment.apps/lf-tx-simulator created route.route.openshift.io/lf-tx-simulator created configmap/lf-client-agent-cm created service/lf-client-agent created deployment.apps/lf-client-agent created route.route.openshift.io/lf-client-agent created","title":"OpenShift Deployment"},{"location":"deployment/#openshift-deployment","text":"We assume you have already Cloud Pak for Integration installed, and you are already logged to the OpenShift console.","title":"OpenShift Deployment"},{"location":"deployment/#pre-requisites","text":"make tool oc CLI Logged to your OpenShift cluster","title":"Pre-requisites"},{"location":"deployment/#using-make","text":"We have a makefile to drive the installation of the demonstration components cd environments make all Example of output namespace/lf-demo created serviceaccount/lf-demo-sa created clusterrolebinding.rbac.authorization.k8s.io/secrets-to-sa created job.batch/cpsecret created --------------------------- ls-demo project created. Using project \"lf-demo\" on server \"https://api.ahsoka.coc-ibm.com:6443\" . Using project \"lf-demo\" on server \"https://api.ahsoka.coc-ibm.com:6443\" . ------------------------------------------------ Create dev Event Streams cluster in lf-demo project ------------------------------------------------ eventstreams.eventstreams.ibm.com/dev created kafkatopic.eventstreams.ibm.com/lf-raw-tx created kafkatopic.eventstreams.ibm.com/lf-tx-a created kafkatopic.eventstreams.ibm.com/lf-tx-b created kafkauser.eventstreams.ibm.com/scram-user created kafkauser.eventstreams.ibm.com/tls-user created ------------------------------------------------ Create dev IBM MQ in lf-demo project ------------------------------------------------ configmap/mq-config created configmap/mq-mqsc-config created queuemanager.mq.ibm.com/lf-demo-mq created ------------------------------------------------ Create dev Kafka Connect cluster in lf-demo project ------------------------------------------------ kafkaconnect.eventstreams.ibm.com/eda-kconnect-cluster created ------------------------------------------------ Deploy MQ Source connector ------------------------------------------------ kafkaconnector.eventstreams.ibm.com/mq-source created configmap/lf-tx-simulator-cm created service/lf-tx-simulator created deployment.apps/lf-tx-simulator created route.route.openshift.io/lf-tx-simulator created configmap/lf-client-agent-cm created service/lf-client-agent created deployment.apps/lf-client-agent created route.route.openshift.io/lf-client-agent created","title":"Using make"},{"location":"design/","text":"Solution Design \u00b6 Simple domain model for client \u00b6 We can use a simple client model to define a life insurance client that can have different benifeciary or transfer the life insurance to other persons. The model can be see as: Life insurance policy can be transferred to a family member or someone else, therefore the model stores not only information about the client to whom the policy belongs but also information about any related people and their relationship to the client. Client information is in Person objecr, but also as a Client . Other people related to the client to whom the policy may be transferred or who may receive the policy benefit upon the client\u2019s death are also Person s. Client Category is to be able to classify client for marketing reason based on demographics and financial details. The remaining two classes are needed for describing the nature of the relationship between clients and other people. Relation types is stored in the ClientRelationType . The ClientRelated instances store references to the client (client_id), the related person (person_id), the nature of that relation (client_relation_type_id), all addition details (details), if any, and a flag indicating whether the relation is currently active (is_active). The java classes for this model are in the lf-tx-simulator project. In the future, can extend this model with the Life Insurance offer and product. Model inspiration is coming from Vertabelo blog Transaction Simulator \u00b6 The code is under the folder lf-tx-simulator and use JMS API to interact with MQ. The main class is in MQProducer.java and expose a send(client) method with the domain class and prepare a transaction event message. The following code extract illustrates this: // this is more a demo trick. Should think of a better implementation TransactionEvent tx = new TransactionEvent (); if ( newClient ) { tx . type = TransactionEvent . TX_CLIENT_CREATED ; } else { tx . type = TransactionEvent . TX_CLIENT_UPDATED ; } tx . payload = client ; tx . txid = client . id ; tx . timestamp = new Date (). getTime (); String msg = parser . writeValueAsString ( tx ); TextMessage message = jmsContext . createTextMessage ( msg ); message . setJMSCorrelationID ( client . id ); producer . send ( destination , message ); The transaction event adds meta data and supports generic payload. We may replace it with CloudEvent . The code is using the JMSCorrelationID to define the potential Key to be used by Kafka producer. We will detail that in next section. The rest of this application exposes a REST resource to control the simulation and sends some new client data or update to existing client's one. The APIs should be enough to demonstrate the needed requirements. The deployment descriptors are in the environements/apps folder The MQ source connector \u00b6 The declaration of the Kafka connect cluster is done in the environements/service folder and the MQ source connector in kafka-mq-src-connector.yaml . The interesting part of the connector configuration is the use of JMS and the JMSCorrelationID as a source for the Kafka Record key. mq.record.builder : com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder mq.connection.mode : client mq.message.body.jms : true mq.record.builder.key.header : JMSCorrelationID The Client event stream processing. \u00b6 The code is in the client-event-processing folder , and supports the implementation of the green component in figure below: The streaming algorithm is quite simple: get continuous update of the category reference data: this should be rare, but the process will get any new updated to those data. This will be a Table in memory and persisted in Kafka to keep only the last update per record key. The table below illustrates the mockup data used: Key Value 1 \"category_name\": \"Personal\" 2 \"category_name\": \"VIP\" 3 \"category_name\": \"Employee\" 4 \"category_name\": \"Business\" Topology code with GlobalKtable so if we partition the input stream we have a unique table. GlobalKTable < Integer , ClientCategory > categories = builder . globalTable ( categoriesInputStreamName , Consumed . with ( Serdes . Integer (), categorySerder ), Materialized . as ( storeSupplier )); Process transaction events in a streams, validate the data, and any transaction in error goes to dead letter queue. { \"id\" : \"101012\" , \"code\" : \"C02\" , \"insuredPerson\" : { \"id\" : 2 , \"code\" : \"P02\" , \"first_name\" : \"julie\" , \"last_name\" : \"thesimmer\" , \"address\" : \"10 market street, CA, San Franciso\" , \"phone\" : \"650-650-650\" , \"mobile\" : \"\" , \"email\" : \"jswimmer@email.com\" }, \"client_category_id\" : 1 } The transaction streaming processing start with the following statements: KStream < String , TransactionEvent > transactions = builder . stream ( transactionsInputStreamName , Consumed . with ( Serdes . String (), transactionEventSerder )); Map < String , KStream < String , TransactionEvent >> branches = transactions . split ( Named . as ( \"A-\" )) . branch (( k , v ) -> transactionNotValid ( v ), Branched . as ( \"error\" )) . defaultBranch ( Branched . as ( \"good-data\" )); This is an exactly once delivery. Transform the input transaction hierarchical model into a flat model: ClientOutput class public class ClientOutput implements JSONSerdeCompatible { public String client_id ; public String client_code ; public Integer client_category_id ; public String client_category_name ; public String first_name ; public String last_name ; public String address ; public String phone ; public String mobile ; public String email ; Data transformation is simple: branches . get ( \"A-good-data\" ) . mapValues ( v -> buildClientOutputFromTransaction ( v )) Enrich with the category name by doing a join with the categories table . join ( categories , ( txid , co ) -> co . client_category_id , //When you join a stream and a table, you get a new stream ( oldOutput , matchingCategory ) -> new ClientOutput ( oldOutput , matchingCategory . category_name ) ) Route based on category name content to different target. . branch ( ( k , v ) -> v . client_category_name != null && v . client_category_name . equals ( \"Business\" ), Branched . as ( \"category-b\" )) . defaultBranch ( Branched . as ( \"category-a\" )); The deployment descriptors are in the environments/apps/client-event-processing folder >>> Next: demonstration script","title":"Design"},{"location":"design/#solution-design","text":"","title":"Solution Design"},{"location":"design/#simple-domain-model-for-client","text":"We can use a simple client model to define a life insurance client that can have different benifeciary or transfer the life insurance to other persons. The model can be see as: Life insurance policy can be transferred to a family member or someone else, therefore the model stores not only information about the client to whom the policy belongs but also information about any related people and their relationship to the client. Client information is in Person objecr, but also as a Client . Other people related to the client to whom the policy may be transferred or who may receive the policy benefit upon the client\u2019s death are also Person s. Client Category is to be able to classify client for marketing reason based on demographics and financial details. The remaining two classes are needed for describing the nature of the relationship between clients and other people. Relation types is stored in the ClientRelationType . The ClientRelated instances store references to the client (client_id), the related person (person_id), the nature of that relation (client_relation_type_id), all addition details (details), if any, and a flag indicating whether the relation is currently active (is_active). The java classes for this model are in the lf-tx-simulator project. In the future, can extend this model with the Life Insurance offer and product. Model inspiration is coming from Vertabelo blog","title":"Simple domain model for client"},{"location":"design/#transaction-simulator","text":"The code is under the folder lf-tx-simulator and use JMS API to interact with MQ. The main class is in MQProducer.java and expose a send(client) method with the domain class and prepare a transaction event message. The following code extract illustrates this: // this is more a demo trick. Should think of a better implementation TransactionEvent tx = new TransactionEvent (); if ( newClient ) { tx . type = TransactionEvent . TX_CLIENT_CREATED ; } else { tx . type = TransactionEvent . TX_CLIENT_UPDATED ; } tx . payload = client ; tx . txid = client . id ; tx . timestamp = new Date (). getTime (); String msg = parser . writeValueAsString ( tx ); TextMessage message = jmsContext . createTextMessage ( msg ); message . setJMSCorrelationID ( client . id ); producer . send ( destination , message ); The transaction event adds meta data and supports generic payload. We may replace it with CloudEvent . The code is using the JMSCorrelationID to define the potential Key to be used by Kafka producer. We will detail that in next section. The rest of this application exposes a REST resource to control the simulation and sends some new client data or update to existing client's one. The APIs should be enough to demonstrate the needed requirements. The deployment descriptors are in the environements/apps folder","title":"Transaction Simulator"},{"location":"design/#the-mq-source-connector","text":"The declaration of the Kafka connect cluster is done in the environements/service folder and the MQ source connector in kafka-mq-src-connector.yaml . The interesting part of the connector configuration is the use of JMS and the JMSCorrelationID as a source for the Kafka Record key. mq.record.builder : com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder mq.connection.mode : client mq.message.body.jms : true mq.record.builder.key.header : JMSCorrelationID","title":"The MQ source connector"},{"location":"design/#the-client-event-stream-processing","text":"The code is in the client-event-processing folder , and supports the implementation of the green component in figure below: The streaming algorithm is quite simple: get continuous update of the category reference data: this should be rare, but the process will get any new updated to those data. This will be a Table in memory and persisted in Kafka to keep only the last update per record key. The table below illustrates the mockup data used: Key Value 1 \"category_name\": \"Personal\" 2 \"category_name\": \"VIP\" 3 \"category_name\": \"Employee\" 4 \"category_name\": \"Business\" Topology code with GlobalKtable so if we partition the input stream we have a unique table. GlobalKTable < Integer , ClientCategory > categories = builder . globalTable ( categoriesInputStreamName , Consumed . with ( Serdes . Integer (), categorySerder ), Materialized . as ( storeSupplier )); Process transaction events in a streams, validate the data, and any transaction in error goes to dead letter queue. { \"id\" : \"101012\" , \"code\" : \"C02\" , \"insuredPerson\" : { \"id\" : 2 , \"code\" : \"P02\" , \"first_name\" : \"julie\" , \"last_name\" : \"thesimmer\" , \"address\" : \"10 market street, CA, San Franciso\" , \"phone\" : \"650-650-650\" , \"mobile\" : \"\" , \"email\" : \"jswimmer@email.com\" }, \"client_category_id\" : 1 } The transaction streaming processing start with the following statements: KStream < String , TransactionEvent > transactions = builder . stream ( transactionsInputStreamName , Consumed . with ( Serdes . String (), transactionEventSerder )); Map < String , KStream < String , TransactionEvent >> branches = transactions . split ( Named . as ( \"A-\" )) . branch (( k , v ) -> transactionNotValid ( v ), Branched . as ( \"error\" )) . defaultBranch ( Branched . as ( \"good-data\" )); This is an exactly once delivery. Transform the input transaction hierarchical model into a flat model: ClientOutput class public class ClientOutput implements JSONSerdeCompatible { public String client_id ; public String client_code ; public Integer client_category_id ; public String client_category_name ; public String first_name ; public String last_name ; public String address ; public String phone ; public String mobile ; public String email ; Data transformation is simple: branches . get ( \"A-good-data\" ) . mapValues ( v -> buildClientOutputFromTransaction ( v )) Enrich with the category name by doing a join with the categories table . join ( categories , ( txid , co ) -> co . client_category_id , //When you join a stream and a table, you get a new stream ( oldOutput , matchingCategory ) -> new ClientOutput ( oldOutput , matchingCategory . category_name ) ) Route based on category name content to different target. . branch ( ( k , v ) -> v . client_category_name != null && v . client_category_name . equals ( \"Business\" ), Branched . as ( \"category-b\" )) . defaultBranch ( Branched . as ( \"category-a\" )); The deployment descriptors are in the environments/apps/client-event-processing folder >>> Next: demonstration script","title":"The Client event stream processing."},{"location":"eda/","text":"Why event-driven architecture \u00b6 adoption for loosely coupled, event-driven microservice solutions, with new data pipeline used to inject data to modern data lakes, and the adoption of event backbone technology like Apache Kafka, or Apache Pulsar. Event-driven architecture (EDA) is an architecture pattern that promotes the production, detection, consumption of, and reaction to events. It supports asynchronous communication between components and most of the time a pub/sub programming model. The adoption of microservices brings some interesting challenges like data consistency, contract coupling, and scalability that EDA helps to address. From the business value point of view, adopting this architecture helps to scale business applications according to workload and supports easy extension by adding new components over time that are ready to produce or consume events that are already present in the overall system. New real-time data streaming applications can be developed which we were not able to do before. Technical needs \u00b6 At the technical level we can see three adoptions of event-driven solutions: Modern data pipeline to move the classical batch processing of extract, transform and load job to real-time ingestion, where data are continuously visible in a central messaging backbone. The data sources can be databases, queues, or specific producer applications, while the consumers can be applications, streaming flow, long storage bucket, queues, databases\u2026 Adopt asynchronous communication , publish-subscribe protocol between cloud-native microservices to help scaling and decoupling: the adoption of microservices for developing business applications, has helped to address maintenance and scalability, but pure RESTful or SOAP based solutions have brought integration and coupling challenges that inhibited the agility promised by microservice architecture. Pub/sub helps to improve decoupling, but design good practices are very important. See more about EDA advantages for microservices Real time analytics : this embraces pure analytic computations like aggregate on the data streams but also complex event processing, time window-based reasoning, or AI scoring integration on the data streams. >>> Next: Design Read more business requirements","title":"Why EDA now?"},{"location":"eda/#why-event-driven-architecture","text":"adoption for loosely coupled, event-driven microservice solutions, with new data pipeline used to inject data to modern data lakes, and the adoption of event backbone technology like Apache Kafka, or Apache Pulsar. Event-driven architecture (EDA) is an architecture pattern that promotes the production, detection, consumption of, and reaction to events. It supports asynchronous communication between components and most of the time a pub/sub programming model. The adoption of microservices brings some interesting challenges like data consistency, contract coupling, and scalability that EDA helps to address. From the business value point of view, adopting this architecture helps to scale business applications according to workload and supports easy extension by adding new components over time that are ready to produce or consume events that are already present in the overall system. New real-time data streaming applications can be developed which we were not able to do before.","title":"Why event-driven architecture"},{"location":"eda/#technical-needs","text":"At the technical level we can see three adoptions of event-driven solutions: Modern data pipeline to move the classical batch processing of extract, transform and load job to real-time ingestion, where data are continuously visible in a central messaging backbone. The data sources can be databases, queues, or specific producer applications, while the consumers can be applications, streaming flow, long storage bucket, queues, databases\u2026 Adopt asynchronous communication , publish-subscribe protocol between cloud-native microservices to help scaling and decoupling: the adoption of microservices for developing business applications, has helped to address maintenance and scalability, but pure RESTful or SOAP based solutions have brought integration and coupling challenges that inhibited the agility promised by microservice architecture. Pub/sub helps to improve decoupling, but design good practices are very important. See more about EDA advantages for microservices Real time analytics : this embraces pure analytic computations like aggregate on the data streams but also complex event processing, time window-based reasoning, or AI scoring integration on the data streams. >>> Next: Design Read more business requirements","title":"Technical needs"}]}